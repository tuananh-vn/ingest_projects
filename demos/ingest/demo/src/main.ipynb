{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engines:\n",
      "  spark:\n",
      "    config:\n",
      "      jobname: default\n",
      "      master: spark://spark-master:7077\n",
      "      packages:\n",
      "      - mysql:mysql-connector-java:8.0.12\n",
      "    context: spark\n",
      "loggers:\n",
      "  stream:\n",
      "    enable: true\n",
      "    severity: info\n",
      "providers:\n",
      "  ingest:\n",
      "    format: parquet\n",
      "    hostname: hdfs-nn\n",
      "    path: /data/ingest\n",
      "    service: hdfs\n",
      "    write:\n",
      "      coalesce: 2\n",
      "      options:\n",
      "        mode: append\n",
      "        partitionBy:\n",
      "        - date\n",
      "      repartition: 4\n",
      "  source:\n",
      "    database: sakila\n",
      "    hostname: mysql\n",
      "    password: root\n",
      "    port: 3306\n",
      "    read:\n",
      "      cache: true\n",
      "      repartition: 4\n",
      "    service: mysql\n",
      "    username: root\n",
      "resources:\n",
      "  .in:\n",
      "    path: actor\n",
      "    provider: source\n",
      "  .out:\n",
      "    path: actor\n",
      "    provider: ingest\n",
      "    write:\n",
      "      option:\n",
      "        mode: append\n",
      "        partitionBy:\n",
      "        - date\n",
      "run: test\n",
      "variables:\n",
      "  a: 5\n",
      "  b: hello\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dlf.project.workrun('test')\n",
    "dlf.utils.pretty_print(dlf.params.metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_SUBMIT_ARGS:  --packages mysql:mysql-connector-java:8.0.12 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "engine = dlf.engines.get('spark')\n",
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartition  4\n",
      "coalesce  None\n",
      "cache True\n",
      "jdbc:mysql://mysql:3306/sakila\n",
      "+--------+----------+---------+-------------------+\n",
      "|actor_id|first_name|last_name|        last_update|\n",
      "+--------+----------+---------+-------------------+\n",
      "|     105|    SIDNEY|    CROWE|2006-02-15 04:34:33|\n",
      "|     172|   GROUCHO| WILLIAMS|2006-02-15 04:34:33|\n",
      "|      74|     MILLA|   KEITEL|2006-02-15 04:34:33|\n",
      "|      48|   FRANCES|DAY-LEWIS|2006-02-15 04:34:33|\n",
      "|      65|    ANGELA|   HUDSON|2006-02-15 04:34:33|\n",
      "+--------+----------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read by resource alias\n",
    "df_src = engine.read('in')\n",
    "df_src.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_src.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartition  4\n",
      "coalesce  None\n",
      "cache True\n",
      "jdbc:mysql://mysql:3306/sakila\n",
      "+--------+----------+---------+-------------------+\n",
      "|actor_id|first_name|last_name|        last_update|\n",
      "+--------+----------+---------+-------------------+\n",
      "|     105|    SIDNEY|    CROWE|2006-02-15 04:34:33|\n",
      "|     172|   GROUCHO| WILLIAMS|2006-02-15 04:34:33|\n",
      "|      74|     MILLA|   KEITEL|2006-02-15 04:34:33|\n",
      "|      48|   FRANCES|DAY-LEWIS|2006-02-15 04:34:33|\n",
      "|      65|    ANGELA|   HUDSON|2006-02-15 04:34:33|\n",
      "+--------+----------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read by resource path and provider\n",
    "df_src = engine.read(path='actor', provider='source')\n",
    "df_src.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+-------------------+----------+\n",
      "|actor_id|first_name|last_name|        last_update|      date|\n",
      "+--------+----------+---------+-------------------+----------+\n",
      "|     105|    SIDNEY|    CROWE|2006-02-15 04:34:33|2006-02-15|\n",
      "|     172|   GROUCHO| WILLIAMS|2006-02-15 04:34:33|2006-02-15|\n",
      "|      74|     MILLA|   KEITEL|2006-02-15 04:34:33|2006-02-15|\n",
      "|      48|   FRANCES|DAY-LEWIS|2006-02-15 04:34:33|2006-02-15|\n",
      "|      65|    ANGELA|   HUDSON|2006-02-15 04:34:33|2006-02-15|\n",
      "+--------+----------+---------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df = df_src.withColumn('date', F.to_date('last_update'))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartition  4\n",
      "coalesce  2\n",
      "cache False\n",
      "hdfs://hdfs-nn:8020//data/ingest/actor\n"
     ]
    }
   ],
   "source": [
    "# write by resource alias \n",
    "engine.write(df, 'out', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartition  4\n",
      "coalesce  2\n",
      "cache False\n",
      "hdfs://hdfs-nn:8020//data/ingest/actor\n"
     ]
    }
   ],
   "source": [
    "# write by resource path and provider alias\n",
    "engine.write(df, path='actor', provider='ingest', partitionBy=['date'], mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post write checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartition  None\n",
      "coalesce  None\n",
      "cache False\n",
      "hdfs://hdfs-nn:8020//data/ingest/actor\n"
     ]
    }
   ],
   "source": [
    "# read back from hdfs in parquet format\n",
    "df_trg = engine.read('out', 'ingest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df.count()==df_trg.count())\n",
    "assert(df.subtract(df_trg).count()==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data on hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all worked fine up to here, you should be able to see the data at [http://localhost:50070/explorer.html#/data/ingest/actor](http://localhost:50070/explorer.html#/data/ingest/actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
